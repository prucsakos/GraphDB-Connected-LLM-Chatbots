{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52507,"status":"ok","timestamp":1708815677361,"user":{"displayName":"Prucs Ákos","userId":"12241478937817497031"},"user_tz":-60},"id":"nZHHkNbIr4QG","outputId":"d9c35307-e970-442f-eed4-0a7e52ba583b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n","Collecting transformers\n","  Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Installing collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.37.2\n","    Uninstalling transformers-4.37.2:\n","      Successfully uninstalled transformers-4.37.2\n","Successfully installed transformers-4.38.1\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bitsandbytes\n","  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->bitsandbytes) (1.25.2)\n","Installing collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.42.0\n","Collecting flash-attn\n","  Downloading flash_attn-2.5.5.tar.gz (2.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.1.0+cu121)\n","Collecting einops (from flash-attn)\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (23.2)\n","Collecting ninja (from flash-attn)\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n","Building wheels for collected packages: flash-attn\n","  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for flash-attn: filename=flash_attn-2.5.5-cp310-cp310-linux_x86_64.whl size=120352304 sha256=e70f2f0d4fae98c9ff9742404f3e067bbae56bd3212578ff81c30f73c5ae1a15\n","  Stored in directory: /root/.cache/pip/wheels/b2/67/52/8b6d5fcffdd9e1ec868f554cdef8f03eedb4bf4dcac852fca2\n","Successfully built flash-attn\n","Installing collected packages: ninja, einops, flash-attn\n","Successfully installed einops-0.7.0 flash-attn-2.5.5 ninja-1.11.1.1\n"]}],"source":["# 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\n","!pip install -U transformers\n","!pip install -q accelerate\n","!pip install bitsandbytes\n","!pip install flash-attn\n","#!huggingface-cli login\n","\n","from transformers import AutoTokenizer\n","import accelerate\n","import transformers\n","import torch\n","import re\n","\n","context_length = 2000\n","token=\"\""]},{"cell_type":"markdown","metadata":{"id":"Xpgtg1x9YrCT"},"source":["### Llama2 interface"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZHc4Ee8I3hFI"},"outputs":[],"source":["class LlamaInterface():\n","    \"\"\"\n","    Dialouge template:\n","    <s>[INST] <<SYS>>\n","    {{ system_prompt }}\n","    <</SYS>>\n","    [/INST]</s>\n","\n","    {{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s><s>[INST] {{ user_msg_2 }} [/INST]\n","    \"\"\"\n","    def __init__(self, context_length = 2000, token=\"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\"):\n","        self.model = \"meta-llama/Llama-2-7b-chat-hf\"\n","        self.context_length = context_length\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.model, token=token)\n","        self.pipeline = transformers.pipeline(\n","            \"text-generation\",\n","            model=self.model,\n","            torch_dtype=torch.float16,\n","            device_map=\"auto\",\n","            token=token\n","        )\n","\n","    def inference(self, history, sys_msg) -> str:\n","        #print(\"Debug, history:\\n\", history, \"\\nsys_msg:\\n\", sys_msg)\n","        context = self.build_context(history, sys_msg)\n","        #print(\"Final_context:\", context)\n","        sequences = self.pipeline(\n","          context,\n","          do_sample=True,\n","          top_k=10,\n","          num_return_sequences=1,\n","          eos_token_id=self.tokenizer.eos_token_id,\n","          max_length=self.context_length,\n","          )\n","        #print(\"DEBUG history : \", history)\n","        #print(\"DEBUG context : \", context)\n","        #print(\"DEBUG: \\n\",sequences[0]['generated_text'])\n","        response = sequences[0]['generated_text'][len(context):]\n","        return response\n","\n","\n","    def build_context(self, history, sys_msg)->str:\n","        context = \"\"\n","\n","        sys_msg = f\"<s> [INST] <<SYS>> {sys_msg} <</SYS>> [/INST] </s>\"\n","\n","        maxlen = self.context_length - len(sys_msg)\n","\n","        for dialoge_idx in range(len(history), 0, -1):\n","            sender, msg = history[dialoge_idx-1]\n","            if sender == \"user\":\n","                dialoge = f\"<s>[INST] {sender}: {msg} [/INST] \"\n","            else:\n","                dialoge = f\"{sender}: {msg} </s>\"\n","            dialoge += \"\\n\"\n","\n","            if len(context) + len(dialoge) > maxlen:\n","              break\n","            else:\n","              context = dialoge + context\n","\n","        context = sys_msg + \"\\n\" + context\n","\n","        return context\n","\n","model = LlamaInterface(context_length, token)"]},{"cell_type":"markdown","metadata":{"id":"7DGOwDSVYtYs"},"source":["Gemma interface"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["2f349ed273404d919b0ff1e91f4b4969","3182b0bda3f64106ac15774ab92ca003","b2035382ccfa4cbd9b3fe7a654268926","b65315ecb2184e2c80caa4bca01d1eec","409ca0746edd44958433a6ac7bfd10d5","b042a625af3c4b27b405ad01ac51f118","ceb2cf8213624a48b61a550210fcecde","7719e639bb7147b5ad7e166f8f80e576","8b80215db80a4fd8b6907e319a581e23","5d03db8a549f4ab6b8b109a3a159c7ce","251fea22b7b647a2926642d88d06d893"]},"executionInfo":{"elapsed":5073,"status":"ok","timestamp":1708816348869,"user":{"displayName":"Prucs Ákos","userId":"12241478937817497031"},"user_tz":-60},"id":"ByaCuFDB5hvr","outputId":"24eb79a2-4aef-43d1-bf69-0b08488ce18c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f349ed273404d919b0ff1e91f4b4969","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","#quantization_config = BitsAndBytesConfig(\n","#    load_in_4bit=True,\n","#    bnb_4bit_use_double_quant=True,\n","#    bnb_4bit_quant_type=\"nf4\",\n","#    bnb_4bit_compute_dtype=torch.bfloat16\n","#)\n","\n","\n","class GemmaInterface():\n","    def __init__(self, token, context_length = 2000):\n","        self.context_length = context_length\n","        self.tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", token=token)\n","        self.model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\",\n","                                                          device_map=\"auto\",\n"," #                                                         quantization_config=quantization_config,\n","                                                          torch_dtype=torch.float16,\n","                                                          attn_implementation=\"flash_attention_2\",\n","                                                          token=token)\n","\n","\n","\n","    def inference(self, history, sys_msg = \"\") -> str:\n","        history[0] = ('user', sys_msg +'\\n\\n'+ history[0][1])\n","        history = [{'role':'user' if texter=='user' else 'assistant', 'content':message} for texter, message in history]\n","        prompt = self.tokenizer.apply_chat_template(history, tokenize=False, add_generation_prompt=True)\n","        prompt_len = len(prompt)\n","        input_ids = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n","        outputs = self.model.generate(input_ids=input_ids.to(self.model.device),\n","                                      max_new_tokens=512,\n","                                      do_sample=True,\n","  #                                    temperature=0.1,\n","                                      top_k=10\n","                                      )\n","        text = self.tokenizer.decode(outputs[0], skip_special_tokens=False, clean_up_tokenization_spaces=True)\n","        text = text[prompt_len:]\n","        text = text.replace('<eos>', '')\n","        return text\n","\n","model = GemmaInterface(token=token)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11468,"status":"ok","timestamp":1708816032779,"user":{"displayName":"Prucs Ákos","userId":"12241478937817497031"},"user_tz":-60},"id":"JPcCRqszTB8T","outputId":"f7e10acd-baca-4686-e6be-fa6f411f6edd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Node1, Relation, Node2\n","Napoleon, liked, Ice cream\n","Ice Cream, made of, Ice\n","**Coffee Making Process**\n","\n","**1. Bean Selection and Preparation:**\n","* Choose high-quality, freshly roasted coffee beans.\n","* Grind the beans to a consistent size. A fine grind is recommended for optimal extraction.\n","\n","**2. Brewing Method Selection:**\n","* **Drip Coffee Maker:** Uses a filter and hot water to extract coffee grounds.\n","* **French Press:** Uses coarsely ground coffee and hot water in a filter.\n","* **Pour-Over:** Involves pouring hot water over coffee grounds in a filter.\n","* **Cold Brew:** Uses cold water to extract coffee grounds for a smooth and less acidic drink.\n","\n","**3. Brewing:**\n","\n","**Drip Coffee Maker:**\n","* Insert a filter into the portafilter.\n","* Add ground coffee to the filter in a ratio of 1:15-1:20 (grounds to water).\n","* Add hot water (195-205°F/90-96°C) to the brew chamber.\n","* Start the brewing process.\n","\n","**French Press:**\n","* Add coarsely ground coffee to the filter.\n","* Add hot water (195-205°F/90-96°C) to the french press.\n","* Stir gently to dissolve the coffee grounds.\n","* Let the coffee steep for 4-6 minutes.\n","\n","**Pour-Over:**\n","* Use a filter designed for pour-over brewing.\n","* Add ground coffee to the filter in a fine grind.\n","* Slowly pour hot water (195-205°F/90-96°C) over the grounds in a circular motion.\n","* Allow the coffee to brew for 2-4 minutes.\n","\n","**4. Serving:**\n","* Serve the brewed coffee in a mug or cup.\n","* Adjust the brewing time and water temperature to your preference.\n","\n","**Tips for Optimal Coffee:**\n","* Use freshly roasted, high-quality coffee beans.\n","* Use the correct grind size for your chosen brewing method.\n","* Use filtered or bottled water.\n","* Maintain consistent brewing water temperature.\n","* Experiment with different brewing parameters to find your preferred taste.\n","TIME: 11.33286166191101\n"]}],"source":["import time\n","start = time.time()\n","out = model.inference([('user','Your job is to identify knowlege in text and extract data triplets in the form of node1, relation, node2\\\\n.\\n Text: Napoleon liked ice cream. Ice cream is made of ice and cream.')])\n","print(out)\n","out = model.inference([('user','Explain me coffee making.')])\n","print(out)\n","print(\"TIME: \" + str(time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zP-HzM6lr669"},"outputs":[],"source":["\n","class BaseAgent():\n","\n","    BASE_SYS_MSG = \"\"\n","\n","    def __init__(self, inference_model, system_msg : str | None = None):\n","        self.model = inference_model\n","        self.history = []\n","\n","        self.system_msg = self.BASE_SYS_MSG\n","        if system_msg is not None:\n","            self.system_msg = system_msg\n","\n","    def transform_msg(self, msg):\n","        return msg\n","\n","    def reset_history(self):\n","        self.history = []\n","\n","    def message(self, msg : str, new : bool = False) -> str:\n","        if new:\n","            self.reset_history()\n","\n","        self.append_user_msg(msg)\n","\n","        answer = self.model.inference(self.history, self.system_msg)\n","        self.append_system_msg(answer)\n","\n","        return self.transform_msg(answer)\n","\n","    def append_user_msg(self, msg):\n","        self.history.append((\"user\", msg))\n","\n","    def append_system_msg(self, msg):\n","        self.history.append((\"model\", msg))\n","\n","class ConversationalAgent(BaseAgent):\n","\n","    CHAT_SYS_MSG = \"You have received a message from the user. Additionally, relevant information has been retrieved from the database. Using this information, engage in a conversation with the user to solve their problem or answer their query. Provide clear, informative, and contextually relevant responses based on both the user's input and the additional information provided.\"\n","\n","    def __init__(self, inference_model, system_msg : str | None = None):\n","        self.system_msg = self.CHAT_SYS_MSG\n","        if system_msg is not None:\n","            self.system_msg = system_msg\n","\n","        super().__init__(inference_model, self.system_msg)\n","\n","        print(f\"InfoExtracterAgent initialized with system message: {self.system_msg}\")\n","\n","    def message(self, msg : str, data_str : str, new : bool = False) -> str:\n","        if new:\n","            self.reset_history()\n","\n","        inp = f\"Data: {data_str}\\nUser message: {msg}\"\n","        self.append_user_msg(inp)\n","\n","        answer = self.model.inference(self.history, self.system_msg)\n","        self.append_system_msg(answer)\n","\n","        return self.transform_msg(answer)\n","\n","class InfoExtracterAgent(BaseAgent):\n","\n","    EXTRACTER_SYS_MSG = \"\"\"Analyze the user input meticulously. Extract identifiable information and express it as triplets suitable for database storage. Each triplet should capture a key entity, another related entity, and the relationship between them, based on the context provided. Format your output as follows: <entity1>,<relationship>,<entity2>. Separate each triplet with a new line. If the input lacks identifiable information, return an empty response. Your precision in identifying and structuring this information is crucial for accurate knowledge graph construction. Example of expected output for the input 'Alice sent Bob an email about the meeting tomorrow': <Alice>,<Bob>,<recipient> <Alice>,<email>,<subject> <Bob>,<email>,<recipient> <meeting>,<email>,<topic>\"\"\"\n","\n","    def __init__(self, inference_model, system_msg : str | None = None):\n","        self.system_msg = self.EXTRACTER_SYS_MSG\n","        if system_msg is not None:\n","            self.system_msg = system_msg\n","\n","        super().__init__(inference_model, self.system_msg)\n","\n","        print(f\"InfoExtracterAgent initialized with system message: {self.system_msg}\")\n","\n","    def transform_msg(self, msg):\n","        # return <node>,<node>,<relationship>\\n ...\n","        #print(\"DEBUG: \", msg)\n","        lines = msg.split('\\n')\n","        triplets = [self.extract_triplet(line) for line in lines]\n","        triplets = [triplet for triplet in triplets if triplet is not None]\n","\n","        # return List[(n1,n2,r1),...]\n","        return triplets\n","\n","    def extract_triplet(self, s):\n","        # Define the regex pattern for matching triplets inside angle brackets\n","        pattern = r\".*<([^>]+)>, ?<([^>]+)>, ?<([^>]+)>.*\"\n","\n","        # Search the string for the pattern\n","        match = re.match(pattern, s)\n","\n","        # If a match is found, return the three words inside the groups\n","        if match:\n","            return match.groups()\n","        else:\n","            # If no match is found, return None to indicate the string is not valid\n","            return None\n","\n","class InfoQueryAgent(BaseAgent):\n","\n","    #QUERY_SYS_MSG = \"Given the user input, determine if additional information from the database is required to address the query effectively. If so, identify and return a list of keywords - each in new line - that will be used to query the database for relevant information. These keywords should be central to understanding and solving the user's problem. If no additional information is needed, return nothing.\"\n","    QUERY_SYS_MSG = \"Examine the user input to determine if further information from the database is necessary for an effective response. If so, identify and list keywords that will be used to query the database for relevant information. These keywords must be central to understanding and resolving the user's query. List each keyword on a new line. If the input does not necessitate additional information, return an empty response. Your ability to discern key information will enhance the relevance and accuracy of database queries. For instance, if the user asks 'How to care for an orchid?', your response should be: orchid, care instructions.\"\n","    def __init__(self, inference_model, system_msg : str | None = None):\n","        self.system_msg = self.QUERY_SYS_MSG\n","        if system_msg is not None:\n","            self.system_msg = system_msg\n","\n","        super().__init__(inference_model, self.system_msg)\n","\n","        print(f\"InfoQueryAgent initialized with system message: {self.system_msg}\")\n","\n","    def transform_msg(self, msg):\n","        # return list of keywords\n","        words = msg.split(',')\n","        words = [word.strip() for word in words]\n","\n","        return words\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1708816366696,"user":{"displayName":"Prucs Ákos","userId":"12241478937817497031"},"user_tz":-60},"id":"VqJZaBxxr9N8","outputId":"490261cc-7166-43be-9d69-2bc41fd36d43"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('Node1', 'relation1', 'Node2'), ('Node2', 'relation2', 'Node3'), ('Node1', 'relation2', 'Node3'), ('Node3', 'rel4', 'node5')]\n"]}],"source":["# class simple graph db\n","class BaseGraphDB():\n","    def __init__(self):\n","        pass\n","\n","    def insert(self, node1, relation, node2):\n","        pass\n","\n","    def query(self, node, length):\n","        pass\n","\n","class GraphDBSimple():\n","    def __init__(self):\n","        self.graph = {}\n","\n","    def insert(self, node1, relation, node2):\n","        if node1 not in self.graph:\n","            self.graph[node1] = []\n","        self.graph[node1].append((relation, node2))\n","\n","    def query(self, node, length):\n","        result = []\n","        self._dfs(node, length, [], result)\n","        return result\n","\n","    def _dfs(self, node, depth, path, result):\n","        if depth == 0:\n","            result.append(list(path))\n","            return\n","        if node not in self.graph:\n","            return\n","        for relation, next_node in self.graph[node]:\n","            path.append((node, relation, next_node))\n","            self._dfs(next_node, depth-1, path, result)\n","            path.pop()\n","\n","\n","def flatten_list(nested_list):\n","    \"\"\"Flattens a nested list of any depth.\"\"\"\n","    for element in nested_list:\n","        if isinstance(element, list):\n","            yield from flatten_list(element)\n","        else:\n","            yield element\n","\n","# Example usage:\n","db = GraphDBSimple()\n","db.insert('Node1', 'relation1', 'Node2')\n","db.insert('Node1', 'relation2', 'Node3')\n","db.insert('Node2', 'relation2', 'Node3')\n","db.insert('Node3', 'rel4', 'node5')\n","db.insert('backnode10', 'rel10', 'Node1')\n","print(list(flatten_list(db.query('Node1', 2))))\n","del db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vgwdkr5qwXvx"},"outputs":[],"source":["context_length = 2000\n","query_depth = 2\n","\n","class MultiagentChatBot():\n","    def __init__(self, model, db):\n","        self.model = model\n","        self.agent_databuilder = InfoExtracterAgent(model)\n","        self.agent_datareader = InfoQueryAgent(model)\n","        self.agent_conversational = ConversationalAgent(model)\n","\n","        self.db = db\n","\n","    def message(self, msg, debug=False):\n","\n","        # extract data\n","        triplets = self.agent_databuilder.message(msg, new=True) # return [(n1,r1,n2), ...]\n","        [self.db.insert(t[0], t[1], t[2]) for t in triplets]\n","\n","        if debug:\n","            print(f\"Triplets: {triplets}\")\n","\n","        # query data\n","        keywords = self.agent_datareader.message(msg, new=True)\n","        data = list(flatten_list([self.db.query(keyword, query_depth) for keyword in keywords]))\n","        data_str = '\\n'.join([str(d) for d in data])\n","\n","        if debug:\n","            print(f\"data_Str: {data_str}\")\n","\n","        output = self.agent_conversational.message(msg, data_str)\n","\n","        return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4857,"status":"ok","timestamp":1708816418122,"user":{"displayName":"Prucs Ákos","userId":"12241478937817497031"},"user_tz":-60},"id":"xgER26ANLvN2","outputId":"8386a08d-ed73-4d83-da23-25c654b22f4b"},"outputs":[{"name":"stdout","output_type":"stream","text":["InfoExtracterAgent initialized with system message: Analyze the user input meticulously. Extract identifiable information and express it as triplets suitable for database storage. Each triplet should capture a key entity, another related entity, and the relationship between them, based on the context provided. Format your output as follows: <entity1>,<relationship>,<entity2>. Separate each triplet with a new line. If the input lacks identifiable information, return an empty response. Your precision in identifying and structuring this information is crucial for accurate knowledge graph construction. Example of expected output for the input 'Alice sent Bob an email about the meeting tomorrow': <Alice>,<Bob>,<recipient> <Alice>,<email>,<subject> <Bob>,<email>,<recipient> <meeting>,<email>,<topic>\n","InfoQueryAgent initialized with system message: Examine the user input to determine if further information from the database is necessary for an effective response. If so, identify and list keywords that will be used to query the database for relevant information. These keywords must be central to understanding and resolving the user's query. List each keyword on a new line. If the input does not necessitate additional information, return an empty response. Your ability to discern key information will enhance the relevance and accuracy of database queries. For instance, if the user asks 'How to care for an orchid?', your response should be: orchid, care instructions.\n","InfoExtracterAgent initialized with system message: You have received a message from the user. Additionally, relevant information has been retrieved from the database. Using this information, engage in a conversation with the user to solve their problem or answer their query. Provide clear, informative, and contextually relevant responses based on both the user's input and the additional information provided.\n","Triplets: []\n","data_Str: None\n","None\n","None\n","None\n","None\n","None\n","Based on the user message, we can conclude that they are interested in learning about the process of making coffee. We can offer the following information:\n","\n","* Coffee is typically made by brewing roasted coffee beans in hot water.\n","* The water needs to be filtered to remove impurities.\n","* Coffee is often added with milk, sugar, or flavorings.\n","* The brewing time and water amount will vary depending on the desired strength of the coffee.\n","\n","We can provide further details by explaining the different types of coffee beans, brewing methods, and the importance of proper coffee maintenance. We can also suggest some resources for learning more about coffee brewing, such as online tutorials or coffee shops.\n"]}],"source":["#model = LlamaInterface(context_length) model initialized at top\n","db = BaseGraphDB()\n","chatbot = MultiagentChatBot(model, db)\n","out = chatbot.message(\"Hey, I know that Napoleon liked coffee. I wonder how that is made?\", debug=True)\n","print(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-efe1On8MrCw"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwjxZpmhNU0K"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNnykNwZ2wqDwwAsuya5Ari","gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"251fea22b7b647a2926642d88d06d893":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f349ed273404d919b0ff1e91f4b4969":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3182b0bda3f64106ac15774ab92ca003","IPY_MODEL_b2035382ccfa4cbd9b3fe7a654268926","IPY_MODEL_b65315ecb2184e2c80caa4bca01d1eec"],"layout":"IPY_MODEL_409ca0746edd44958433a6ac7bfd10d5"}},"3182b0bda3f64106ac15774ab92ca003":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b042a625af3c4b27b405ad01ac51f118","placeholder":"​","style":"IPY_MODEL_ceb2cf8213624a48b61a550210fcecde","value":"Loading checkpoint shards: 100%"}},"409ca0746edd44958433a6ac7bfd10d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d03db8a549f4ab6b8b109a3a159c7ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7719e639bb7147b5ad7e166f8f80e576":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b80215db80a4fd8b6907e319a581e23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b042a625af3c4b27b405ad01ac51f118":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2035382ccfa4cbd9b3fe7a654268926":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7719e639bb7147b5ad7e166f8f80e576","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b80215db80a4fd8b6907e319a581e23","value":2}},"b65315ecb2184e2c80caa4bca01d1eec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d03db8a549f4ab6b8b109a3a159c7ce","placeholder":"​","style":"IPY_MODEL_251fea22b7b647a2926642d88d06d893","value":" 2/2 [00:02&lt;00:00,  1.21s/it]"}},"ceb2cf8213624a48b61a550210fcecde":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
