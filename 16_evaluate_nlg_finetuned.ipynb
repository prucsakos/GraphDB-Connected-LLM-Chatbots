{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyOy/wQFQPCsF31cSL8oSoN/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a44e84126c984fcea5db9735810a7f3b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75f0ecd14bf04f288b64a03f8b6243b9","IPY_MODEL_b674fe413c1c424aa145bcd690922cb4","IPY_MODEL_7eba459b8a5c4bad8f9571082b66970b"],"layout":"IPY_MODEL_f978e6ae7cc34361976e036933bc7ecf"}},"75f0ecd14bf04f288b64a03f8b6243b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a226ce32caf5448cb5cc0d25d1ebb4bf","placeholder":"​","style":"IPY_MODEL_f9f48c1df18c42b1b1cd30c66312dd11","value":"Loading checkpoint shards: 100%"}},"b674fe413c1c424aa145bcd690922cb4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_909891d3eca24feea767eaa8dc83de8d","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd47ac4092f64529ba038b973972a517","value":2}},"7eba459b8a5c4bad8f9571082b66970b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f33102925b14a15af4e698b1c056e3f","placeholder":"​","style":"IPY_MODEL_57cc60352fc7496eab7d104de2a82331","value":" 2/2 [00:02&lt;00:00,  1.14it/s]"}},"f978e6ae7cc34361976e036933bc7ecf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a226ce32caf5448cb5cc0d25d1ebb4bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9f48c1df18c42b1b1cd30c66312dd11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"909891d3eca24feea767eaa8dc83de8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd47ac4092f64529ba038b973972a517":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f33102925b14a15af4e698b1c056e3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57cc60352fc7496eab7d104de2a82331":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1sY-pCT6Fog2","executionInfo":{"status":"ok","timestamp":1714396984545,"user_tz":-120,"elapsed":111147,"user":{"displayName":"Prucs Ákos","userId":"12241478937817497031"}},"outputId":"8b545300-046f-479e-e08c-3713da65a747"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip3 install -q -U bitsandbytes\n","!pip3 install -q -U accelerate==0.27.1\n","!pip3 install -q -U peft==0.8.2\n","!pip3 install -q -U trl==0.7.10\n","!pip3 install -q -U datasets==2.17.0\n","!pip3 install -q -U transformers==4.38.0\n","!pip3 install -q -U rouge_score"]},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n","\n","import gc\n","import tqdm\n","import copy\n","import time\n","import random\n","import torch\n","from datasets import load_dataset, load_metric\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import pandas as pd"],"metadata":{"id":"eactQ0d4F1fk","executionInfo":{"status":"ok","timestamp":1714398447501,"user_tz":-120,"elapsed":4624,"user":{"displayName":"Prucs Ákos","userId":"12241478937817497031"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["train_set = load_dataset(\"rPucs/TripletDollyQA-3k-Gemma-Nlg\", \"train\")[\"train\"]\n","test_set = load_dataset(\"rPucs/TripletDollyQA-3k-Gemma-Nlg\", \"test\")[\"train\"]\n","test_set = test_set.select(range(0,100))\n","\n","### avg response len for improved generation w/ selection\n","train_response_lens = [len(datapoint[\"response\"]) for datapoint in train_set]\n","num_sample = len(train_response_lens)\n","avg_train_response_len = sum([len/num_sample for len in train_response_lens])\n","print(avg_train_response_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"thqwmMtUTHKr","executionInfo":{"status":"ok","timestamp":1714398452279,"user_tz":-120,"elapsed":4781,"user":{"displayName":"Prucs Ákos","userId":"12241478937817497031"}},"outputId":"52885dbd-3ff1-488c-f96f-5833163f4680"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["288.5537500000003\n"]}]},{"cell_type":"code","source":["def formatting_prompts_func(dataset):\n","    output_texts = []\n","    for datapoint in dataset:\n","        text = f\"### User input: {datapoint['instruction']} \\n### Relations: {' '.join(['<entry><head>{}<rel>{}<tail>{}'.format(rel['head'], rel['type'], rel['tail']) for rel in datapoint['context']]) } \\n### Response:{datapoint['response']}<eos>\"\n","        output_texts.append(text)\n","    return output_texts\n","def get_completion(prompt: str, model, tokenizer) -> str:\n","  device = \"cuda:0\"\n","  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n","  model_inputs = encodeds.to(device)\n","  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n","  # decoded = tokenizer.batch_decode(generated_ids)\n","  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n","  return (decoded)\n","\n","def get_completion_batch(prompt: list[str], model, tokenizer, num_samples=1, mid_length=avg_train_response_len) -> list[str]:\n","  device = \"cuda:0\"\n","  batch_size = len(prompt)\n","  encodeds = tokenizer(prompt,\\\n","                      return_tensors=\"pt\", add_special_tokens=True,\\\n","                      padding=True)\n","  model_inputs = encodeds.to(device)\n","  if num_samples > 1:\n","    generated_ids = model.generate(**model_inputs, max_new_tokens=600, do_sample=True, num_return_sequences=num_samples) #, pad_token_id=tokenizer.eos_token_id)\n","    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","    # restore batches\n","    batch_decoded = [decoded[(idx*num_samples):(idx*num_samples)+num_samples] for (idx, decoded) in list(zip(range(batch_size), [decoded]*batch_size))] # batch_\n","    # improve by selecting the one that is the closest to the avg len.  - \"readable code xd\"\n","    batch_best_fit_idx = [sorted(list(zip(range(len(decoded)), map(lambda x : abs(len(x) - mid_length), decoded))), key=lambda x : x[1])[0][0] for decoded in batch_decoded] # list of idx of best fit - list(int)\n","    batch_decoded = [decodeds[idx] for (idx, decodeds) in list(zip(batch_best_fit_idx, batch_decoded))]\n","    return batch_decoded\n","  else:\n","    generated_ids = model.generate(**model_inputs, max_new_tokens=600, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n","    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n","    return decoded\n","\n","def rouge_to_pd(r, IDName: str):\n","  \"\"\"\n","  Converts ROUGE scores into a pandas DataFrame.\n","\n","  Parameters:\n","  - r: A dictionary containing ROUGE scores. Each key is a ROUGE method and each value is a tuple of tuples\n","        containing precision, recall, and f-measure for low, mid, and high estimations.\n","  - IDName: A string that identifies the set of ROUGE scores.\n","\n","  Returns:\n","  - A pandas DataFrame with columns for the IDName, method, and each metric (low, mid, high precision, recall, f-measure).\n","  \"\"\"\n","  data = []\n","  # Fixed: Split the string into a list of strings\n","  for method in [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]:\n","    ((low_precision, low_recall, low_fmeasure),\n","      (mid_precision, mid_recall, mid_fmeasure),\n","      (high_precision, high_recall, high_fmeasure)) = r[method]\n","    data.append({\n","        \"IDName\": IDName,\n","        \"method\": method,\n","        \"low_precision\": low_precision,\n","        \"low_recall\": low_recall,\n","        \"low_fmeasure\": low_fmeasure,\n","        \"mid_precision\": mid_precision,\n","        \"mid_recall\": mid_recall,\n","        \"mid_fmeasure\": mid_fmeasure,\n","        \"high_precision\": high_precision,\n","        \"high_recall\": high_recall,\n","        \"high_fmeasure\": high_fmeasure\n","    })\n","  return pd.DataFrame(data)"],"metadata":{"id":"49aEeRJsS875","executionInfo":{"status":"ok","timestamp":1714398499196,"user_tz":-120,"elapsed":330,"user":{"displayName":"Prucs Ákos","userId":"12241478937817497031"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["prompts = formatting_prompts_func(test_set)\n","test_set = test_set.add_column(\"prompt\", prompts)"],"metadata":{"id":"QTJDBtE9GXws","executionInfo":{"status":"error","timestamp":1714398499789,"user_tz":-120,"elapsed":22,"user":{"displayName":"Prucs Ákos","userId":"12241478937817497031"}},"colab":{"base_uri":"https://localhost:8080/","height":304},"outputId":"760c6802-2084-4bf5-f881-e6a637f0b6f8"},"execution_count":7,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"The table can't have duplicated columns but columns ['prompt'] are duplicated.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-561fa80fb33a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatting_prompts_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m         }\n\u001b[1;32m    557\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36madd_column\u001b[0;34m(self, name, column, new_fingerprint)\u001b[0m\n\u001b[1;32m   5630\u001b[0m         \"\"\"\n\u001b[1;32m   5631\u001b[0m         \u001b[0mcolumn_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInMemoryTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pydict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5632\u001b[0;31m         \u001b[0m_check_column_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcolumn_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5633\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5634\u001b[0m         \u001b[0;31m# Concatenate tables horizontally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_check_column_names\u001b[0;34m(column_names)\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0mduplicated_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The table can't have duplicated columns but columns {duplicated_columns} are duplicated.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The table can't have duplicated columns but columns ['prompt'] are duplicated."]}]},{"cell_type":"code","source":["model_ids = [\n","  \"rPucs/gemma-2b-itTripletDolly-WebNLG\",\n","  \"rPucs/gemma-2b-itTripletDolly-WebNLG-fullcollator\",\n","  \"rPucs/gemma-7b-itTripletDolly-WebNLG-fullcollator\"\n","]"],"metadata":{"id":"XJm3JPYPGYNj","executionInfo":{"status":"ok","timestamp":1714398501343,"user_tz":-120,"elapsed":2,"user":{"displayName":"Prucs Ákos","userId":"12241478937817497031"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def evaluate(model_id):\n","  print(\"start evaluating {}...\".format(model_id))\n","\n","  bnb_config = BitsAndBytesConfig(\n","      load_in_4bit=True,\n","      bnb_4bit_use_double_quant=True,\n","      bnb_4bit_quent_type=\"nf4\",\n","      bnb_4bit_compute_dtype=torch.bfloat16\n","  )\n","  model = AutoModelForCausalLM.from_pretrained(model_id,\n","                                                quantization_config=bnb_config,\n","                                                device_map=\"auto\")\n","  tokenizer = AutoTokenizer.from_pretrained(model_id,\n","                                            add_eos_token=True,\n","                                            padding_side=\"left\"\n","                                            )\n","  batch_size = 2\n","  model_responses = []\n","  true_responses = []\n","\n","  for i in tqdm.tqdm(range(0, test_set.num_rows, batch_size)):\n","    to = min(test_set.num_rows, i+batch_size)\n","    if i == to:\n","      continue\n","    batch_prompt = test_set[i:i+batch_size][\"prompt\"]\n","    expected_outputs = [p.split(\"\\n### Response:\")[1] for p in batch_prompt]\n","\n","\n","    batch_input = [p.split(\"\\n### Response:\")[0] + \"\\n### Response:\"  for p in batch_prompt]\n","    completions = get_completion_batch(batch_input , model, tokenizer, num_samples=5)\n","    completions = [c.split(\"\\n### Response:\")[1] for c in completions]\n","\n","    model_responses += completions\n","    true_responses += expected_outputs\n","\n","  del model, tokenizer\n","  torch.cuda.empty_cache()\n","\n","  #test_set = test_set.add_column(\"model_reponse\",  model_responses)\n","  #test_set = test_set.add_column(\"true_response\",  true_responses)\n","\n","  print(\"Print out examples from {}\".format(model_id))\n","  for idx in random.sample(range(len(test_set)), k=2):\n","    print(idx)\n","    print(\"true_response\", true_responses[idx])\n","    print(\"model_reponse\", model_responses[idx])\n","\n","  evaluation_name_id= \"rouge_score_\" + model_id.split(\"/\")[1]\n","\n","  rouge = load_metric('rouge')\n","  references = true_responses\n","  predictions = model_responses\n","  results = rouge.compute(predictions=predictions, references=references)\n","\n","  df = rouge_to_pd(results, evaluation_name_id)\n","  df.to_csv(evaluation_name_id + \".csv\", index=False)\n"],"metadata":{"id":"zSbehTskGfUL","executionInfo":{"status":"ok","timestamp":1714398502109,"user_tz":-120,"elapsed":3,"user":{"displayName":"Prucs Ákos","userId":"12241478937817497031"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["for model_id in model_ids:\n","  try:\n","    evaluate(model_id)\n","  except Exception as e:\n","    print(f\"An error occurred when running:{model_id}: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["a44e84126c984fcea5db9735810a7f3b","75f0ecd14bf04f288b64a03f8b6243b9","b674fe413c1c424aa145bcd690922cb4","7eba459b8a5c4bad8f9571082b66970b","f978e6ae7cc34361976e036933bc7ecf","a226ce32caf5448cb5cc0d25d1ebb4bf","f9f48c1df18c42b1b1cd30c66312dd11","909891d3eca24feea767eaa8dc83de8d","cd47ac4092f64529ba038b973972a517","7f33102925b14a15af4e698b1c056e3f","57cc60352fc7496eab7d104de2a82331"]},"id":"6FsRMTQPLdfP","outputId":"acb26f37-9cd3-40a5-85b5-87e9041a4765"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["start evaluating rPucs/gemma-2b-itTripletDolly-WebNLG...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a44e84126c984fcea5db9735810a7f3b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":[" 76%|███████▌  | 38/50 [10:24<03:09, 15.79s/it]"]}]},{"cell_type":"code","source":["from google.colab import files\n","files.download('/content/rouge_score_gemma-7b-itTripletDolly-WebNLG-fullcollator.csv')"],"metadata":{"id":"4IIJkCc6RuMb","executionInfo":{"status":"aborted","timestamp":1714398272679,"user_tz":-120,"elapsed":4,"user":{"displayName":"Prucs Ákos","userId":"12241478937817497031"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FFfhbNPMBRdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xpLdY_XjBTOy"},"execution_count":null,"outputs":[]}]}